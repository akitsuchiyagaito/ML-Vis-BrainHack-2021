{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data pre-processing and pipelines\n",
    "\n",
    "In this notebook, we will review:\n",
    "- Some common pre-processing steps for ML analyses in Cognitive Neuroscience, and why these are useful/needed.\n",
    "- \n",
    "- !! Explain that common pre-processing steps are so called transformers in scikit-learn, and transformers are also estimators.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler\n",
    "\n",
    "(...)\n",
    "- Explain what standarizing your data means.\n",
    "- Some models like support vector machine classifiers benefit from the data being standarized.\n",
    "    - Talk about svm and mention we will not discuss them in detal, but they can read more about them (here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standarize our data using scikit-learn. For this example we will use the wine dataset of scikit-learn. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine(as_frame=True)\n",
    "dataset['frame']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read the description of the dataset that sklearn provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the dataset reveals that:\n",
    "1. The example contains three clases as targets of predictions. That means, it is a multi-class classification problem. \n",
    "2. We are trying to predict the class of wine using 13 features.\n",
    "3. If you take a look at the mean and standard deviation of these 13 features, you will notice their scale is different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will thus scale each of the features so that they have a mean of 0 and a standard deviation of 1. In scikit-learn we can achieve this by calling the `StandardScaler` class (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define X and y\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Create and fit scaler\n",
    "scaler = StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for fitting the scaler we only need the input features (`X`), and not the target (`y`). But calling the `fit` method is not enough to transform our input to the model. We also need to call the `tranform` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X\n",
    "X_tr = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only tranform the input `X`. Also, we assign to the output of the transformation a different variable than `X` (in this case we used `X_tr`), otherwise the method won't give us the expected outcome. Let's make sure the data got scaled and get the mean and standard deviation for each feature, which should be 0 and 1 respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute mean and std of features\n",
    "means = np.mean(X_tr, axis=0)\n",
    "stds = np.std(X_tr, axis=0)\n",
    "\n",
    "print(f\"means: {np.round(means, 2)}\")\n",
    "print(f\"stds: {np.round(stds, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we stated before, support vector machines can be impacted by the scaling of the data. Let's compare the performance of such model with and without the data scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and fit model with original data\n",
    "svc = SVC().fit(X, y)\n",
    "\n",
    "# Create and fit model with transformed data\n",
    "svc_scaled_data = SVC().fit(X_tr, y)\n",
    "\n",
    "# Print scores of both models\n",
    "print(f\"SVC accuracy with non-scaled data: {np.round(svc.score(X, y), 2)}\")\n",
    "print(f\"SVC accuracy with non-scaled data: {np.round(svc_scaled_data.score(X_tr, y), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the performance of the support vector classifier increased when we scaled our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "The fitting and transformation steps can be simplified using `fit_transform`. Read how to use it [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and implement `StandardScaler` this way. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "- !! Explain what dimensionality reduction is\n",
    "- !! Explain why do it (curse of dimensionality)\n",
    "    - Mention this can lead to fitting our model to noise, but we will explain this concept in detail in the next notebook\n",
    "    - With this approach we can remove noisy data that might be affecting the performance of our model\n",
    "\n",
    "- There are two main types of dimensionality reduction: \n",
    "    - _Feature selection_: We select a subset of our features based on some method.\n",
    "    - _Feature extraction_: We create new (and usually fewer) features based on the existing ones.\n",
    "\n",
    "We will now see some examples of feature selection and feature extraction, and how to compute them using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "In feature selection, we select of subset of the feature columns present in `X`. We perform this subselection based on some method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn we find many methods to perform feature selection, as can be seen [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection). In this example, we will illustrate how `SelectKBest` works (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)).\n",
    "\n",
    "`SelectKBest` selects the __k__ features that have the highest score when evaluated with a pre-defined scoring function. Scikit-learn provides many different types of scoring functions that can be used, and by default uses the ANOVA F-value of the sample (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)).\n",
    "\n",
    "Let's select the 5 features with the highest F-value using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Create and fit selector\n",
    "selector = SelectKBest(k=5).fit(X_tr, y)\n",
    "\n",
    "# Transform data\n",
    "X_sel = selector.transform(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the `selector` object after fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(selector).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that `X_sel` has now less feature columns than `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns in original data: {X.shape[1]}\")\n",
    "print(f\"Number of columns in data after feature selection: {X_sel.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Use another score metric in `SelectKBest` for selecting the $k$ features with the best performance. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Create and fit selector\n",
    "selector = SelectKBest(score_func=chi2, k=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "- We tranform our features into a new set of features living in a lower dimensional space\n",
    "- There are linear/non-linear methods\n",
    "- !! Explain PCA (link to notebook?) (link to documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, dimensionality reduction methods are transformer objects. In this example, we will implement `PCA` (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)) to perform dimensionality reduction and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Let's first create a classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_features=300, \n",
    "    n_informative=5, n_redundant=100, \n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute `PCA` over `X`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=5, random_state=0).fit(X)\n",
    "\n",
    "# Transform data\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! We don't have time to explain the inner outputs of PCA and their rationale, but you can read more about them in detail here (link to notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "{To-Do}\n",
    "\n",
    "Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "- The convenience of pipelines\n",
    "- How to implement a pipeline in sklearn\n",
    "- Creating a pipeline is also an useful tool for avoiding leaking information when splitting the data into training/testing sets or doing cross-validation, but this will be explained in the next chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation of pipeline can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Fit and print estimators\n",
    "pipe = pipe.fit(X, y)\n",
    "\n",
    "# Score model\n",
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `make pipeline` for a faster approach (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)) to creating a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "{To-Do}\n",
    "Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✏️ Check your knowledge\n",
    "\n",
    "Load the ABIDE 2 dataset and create two pipelines:\n",
    "1. _Pipeline 1_: Standarize your data, perform `PCA` selecting $k$ components, and perform classification analysis using `SVC`.\n",
    "2. _Pipeline 2_: Standarize your data, select $k$ best features using F-score, and perform classification analysis using `SVC`.\n",
    "\n",
    "Answer the following questions:\n",
    "1. Which pipeline achieves the best performance?\n",
    "2. Vary $k$. How does the performance change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
