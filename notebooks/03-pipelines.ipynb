{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data pre-processing and pipelines\n",
    "\n",
    "In this notebook, we will review:\n",
    "- Transformer objects in _scikit-learn_.\n",
    "- The concepts of preprocessing, feature selection and feature extraction; some examples of them and how to implement them using_scikit-learn_.\n",
    "- Pipelines in _scikit-learn_ and how to create them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Many real-world datasets don't come prepared for serving as the input of machine learning (ML) analysis and need additional __preprocessing__. For example, in classification problems it is very common that the labels are provided in string format (e.g. `dog`, `cat`) instead of the integers the algorithms require. Fortunately, _scikit-learn_ provides many different functions to transform these datasets and make them ready for analyses. These type of functions belong to a broader type of estimators called __transformers__. \n",
    "\n",
    "Transformers are estimator objects that besides learning from the data, they can also transform it in some way. Besides preprocessing, transformers can also be methods that perform __feature selection__, or __feature engineering/extraction__ steps. \n",
    "\n",
    "In feature selection, we select a subset of the feature columns present in `X`. In feature engineering/extraction, we create a new set of features from the existing ones. _Scikit-learn_ provides many different methods for achieving those aims.\n",
    "\n",
    "Both methods come in useful when we want to perform __dimensionality reduction__ to our data. \n",
    "\n",
    "(...)\n",
    "- Explain why do it (curse of dimensionality)\n",
    "- Curse of dimensionality can lead to fitting our model to noise, but we will explain this concept in detail in the next notebook\n",
    "- With this approach we can remove noisy data that might be affecting the performance of our model\n",
    "\n",
    "\n",
    "This notebook will show you how to implement transformers in _scikit-learn_ based on some of these examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler\n",
    "A popular transformation consists of standarizing the dataset. This removes the mean (also called __centering__) and scales to unit variance (`std=1`) each feature. This is an important step when dealing with data where each feature is scaled differently, and/or where the observations in each feature might not follow a normal standard distribution. These properties can make some algorithms behave badly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn how to standarize our data. For this example we will use a popular dataset stored in _scikit-learn_. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load and print dataset\n",
    "dataset = load_wine(as_frame=True)\n",
    "dataset['frame']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this dataset encoding? We can read a description of the dataset in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataset we will be trying to predict the wine class from some of their chemical/structural attributes. The description of the dataset above particularly reveals that:\n",
    "1. The example contains three clases as targets of predictions. That means, it is a multi-class classification problem. \n",
    "2. We are trying to predict the class of wine using 13 features.\n",
    "3. If you take a look at the mean and standard deviation of these 13 features, you will notice they are scaled differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will thus scale each of the features so that they have a mean of 0 and a standard deviation of 1. In _scikit-learn_ we can achieve this using `StandardScaler` (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define X and y\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Create and fit scaler\n",
    "scaler = StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for fitting the scaler we only need the input features (`X`), and not the target (`y`). But calling the `fit` function is not enough to transform our input to the model. For this we need to use the `transform` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X\n",
    "X_tr = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only tranform the input `X`. Also, we assign to the output of the transformation a different variable than `X` (in this case we used `X_tr`), otherwise the method won't give us the expected outcome. Let's make sure the data got scaled and get the mean and standard deviation for each feature, which should be 0 and 1 respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute mean and std of features\n",
    "means = np.mean(X_tr, axis=0)\n",
    "stds = np.std(X_tr, axis=0)\n",
    "\n",
    "print(f\"means: {np.round(means, 2)}\")\n",
    "print(f\"stds: {np.round(stds, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One type of models that can behave badly if features are not standarized are support vector machines (SVM) classifiers, commonly used in cognitve neuroscience research. We won't describe what these models are in detail, but you can read more about them here {add}.\n",
    "\n",
    "Let's compare the performance of a support vector classifier (implemented with `SCV` in _scikit-learn_) trained with a non-scaled dataset, with that trained on a scaled one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and fit model with original data\n",
    "svc = SVC().fit(X, y)\n",
    "\n",
    "# Create and fit model with transformed data\n",
    "svc_scaled_data = SVC().fit(X_tr, y)\n",
    "\n",
    "# Print scores of both models\n",
    "print(f\"SVC accuracy with non-scaled data: {np.round(svc.score(X, y), 2)}\")\n",
    "print(f\"SVC accuracy with non-scaled data: {np.round(svc_scaled_data.score(X_tr, y), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the performance of SVC increased when we scaled our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "The fitting and transformation steps can be simplified using `fit_transform`. Read how to use it [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and implement `StandardScaler` this way. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select K best\n",
    "\n",
    "`SelectKBest` is one type of feature selection step. It selects the $k$ features that have the best score when evaluated with a pre-defined performance metric. _Scikit-learn_ provides many different types of scoring functions that can be used, and by default uses the ANOVA F-value of the sample (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)).\n",
    "\n",
    "Let's select the 5 features with the highest F-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Create and fit selector\n",
    "selector = SelectKBest(k=5).fit(X_tr, y)\n",
    "\n",
    "# Transform data\n",
    "X_sel = selector.transform(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the `selector` object after fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(selector).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the scores obtained by each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that `X_sel` has now less feature columns than `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns in original data: {X.shape[1]}\")\n",
    "print(f\"Number of columns in data after feature selection: {X_sel.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Use another score metric in `SelectKBest` for selecting the $k$ features with the best performance. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer (example using chi squared)\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Create and fit selector\n",
    "selector = SelectKBest(score_func=chi2, k=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Optional material:_ Principal Component Analysis (PCA)\n",
    "\n",
    "Another type of dimensionality reduction technique common in cognitive neuroscience research is Principal Component Analysis (PCA). If you have time after finishing this notebook, you can read more about it and learn how to implement it using _scikit-learn_ [here](./optional/pca.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "[Pipelines](https://scikit-learn.org/stable/modules/compose.html) are objects in _scikit-learn_ that chain together transformers and a final estimator in a convenient manner.\n",
    "\n",
    "Creating a pipeline is also an useful tool for avoiding leaking information when splitting the data into training/testing sets or doing cross-validation. Don't worry if you don't know what these terms mean, they will be explained in [Notebook 4](./04-preventing_overfitting.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a pipeline that chains together a standard scaler and a support vector classifier model, using the class `Pipeline` (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this pipeline to transform, fit and score our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Fit and print estimators\n",
    "pipe = pipe.fit(X, y)\n",
    "\n",
    "# Score model\n",
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "We can also use `make pipeline` for a faster approach to creating a pipeline. Read the documentation of this method [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline) and implement it to create a pipeline. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC())\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✏️ Check your knowledge\n",
    "\n",
    "Load the ABIDE 2 dataset and create two pipelines:\n",
    "1. _Pipeline 1_: Standarize your data, and perform classification analysis using `SVC`.\n",
    "2. _Pipeline 2_: Standarize your data, select $k$ best features using F-score, and perform classification analysis using `SVC`.\n",
    "\n",
    "Answer the following questions:\n",
    "1. Which pipeline achieves the best performance?\n",
    "2. Vary $k$. How does the performance change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
