{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preventing overfitting\n",
    "\n",
    "In this notebook, we will review:\n",
    "- The overfitting problem.\n",
    "- How to divide your data into a training and testing set with _scikit-learn_.\n",
    "- Concepts of cross-validation and how to implement it with _scikit-learn_.\n",
    "- Different regularization methods.\n",
    "- What is hyper-parameter tuning and how to implement it with _scikit-learn_.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and underfitting\n",
    "\n",
    "Head over to the [](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/03-overfitting.ipynb)\n",
    "\n",
    "- !! Explain what is overfitting and underfitting\n",
    "    - Show images\n",
    "- !! Explain Bias and Variance trade-off\n",
    "    - Show images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing data\n",
    "\n",
    "It is good practice to test the generalizability of the fitted model to real-data generating mechanisms, a process sometimes called __model validation__. In this process we evaluate the performance of the model when predicting data that has never seen during training (but comes from the same distribution as the training data).\n",
    "\n",
    "This leads us to the distinction between __training data__ and __testing data__. We use the training data to fit the parameters of the model. When we evaluate the performance of the model using the testing data, we leave these parameters fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can split our data into training and testing set with _scikit-learn_, and use this process to illustrate the problem of overfitting. \n",
    "\n",
    "First, let's create a fake dataset for classification analysis. We will then fit and score a special kind of support vector classifier that uses a polinomial kernel of degree 4. You don't need to know what this means, besides that this type of model will be more complex and more flexible than a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC mean performance: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=100, n_features=20, n_informative=5, n_redundant=15, random_state=1\n",
    ")\n",
    "\n",
    "# Create and fit SVC\n",
    "svc = SVC(kernel='poly', degree=4, random_state=0).fit(X, y)\n",
    "\n",
    "# Score model\n",
    "print(f\"SVC mean performance: {svc.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the code cell above, and during all the previous examples of this tutorial, we have been using the same dataset for training and evaluating the performance of the model. As explained before, this can result in overestimation of the model perfomance.\n",
    "\n",
    "Let's instead split our dataset into a training and testing set. We can do so using the function `train_test_split` in _scikit-learn_ (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X and y into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the shapes of the training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (75, 20)\n",
      "Shape of training labels: (75,)\n",
      "Shape of testing sett: (25, 20)\n",
      "Shape of testing labels: (25,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of training set: {X_train.shape}\")\n",
    "print(f\"Shape of training labels: {y_train.shape}\")\n",
    "print(f\"Shape of testing sett: {X_test.shape}\")\n",
    "print(f\"Shape of testing labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, _scikit-learn_ splits the dataset into a training set containing 75% of the original data, and a testing size containing the remaining 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we fit our estimator to the training set, and evaluate it both using the training and testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC mean accuracy on training data: 0.84\n",
      "SVC mean accuracy on testing data: 0.52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fit model to training set\n",
    "svc = SVC(kernel='poly', degree=5, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model with training and testing data\n",
    "## subsample training data for unbiased estimate\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(np.arange(len(X_train)), size=25)\n",
    "## print scores\n",
    "print(f\"SVC mean accuracy on training data: {svc.score(X_train[idx], y_train[idx])}\")\n",
    "print(f\"SVC mean accuracy on testing data: {svc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performance of the model drops significantly when evaluated on the testing set, indicating that our model may have overfitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_s_curve\n",
    "# from sklearn.linear_model import make\n",
    "\n",
    "# # Create fake dataset\n",
    "# X, y = make_s_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Suppose we want our testing set size to comprise 20% of the original dataset. Modify `train_test_split` to achieve this aim. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of testing set: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=0)\n",
    "\n",
    "print(f\"Proportion of testing set: {X_test.shape[0]/len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "One approach for performing model validation and prevent overestimating the real performance of the model is __cross validation__. In cross-validation we divide our sample dataset into different subsets. We then perform multiple rounds of training and validation (testing) of the model. In each round, we train on some of those subsets, and validate the model on the remaining ones. Importantly, the assignment of the subsets to training and testing sets rotates.\n",
    "\n",
    "There are many types of cross-validation, but in this tutorial we will use as an example __Stratified K-Fold cross-validation__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold cross-validation\n",
    "\n",
    "One popular type of cross-validation is __K-Fold__ cross-validation. In K-Fold cross-validation the dataset is split into $k$ equal subsets, also called folds. We then perform `k` rounds where each subset is used to validate the model while the others are used for training. Each subset is used for validation only once. This is better illustrated with the following image taken from the excellent blogpost [\"Evaluate a model\"](https://sebastianraschka.com/faq/docs/evaluate-a-model.html) written Sebastian Raschka:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../images/kfoldcv.png\" alt=\"kfoldcv\" width=\"700\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Another type of cross-validation is __Stratified K-Fold__. It follows the same logic of K-Fold cross-validation, but with this technique we make sure that each fold of the dataset has the same proportion of samples for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement Stratified K-Fold in _scikit learn_. We will first create some fake data for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500, n_features=300, n_informative=100, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a stratified cross-validation object using `StratifiedKFold` (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)). We will use 3 folds (defined here as `n_splits`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create cross validation object\n",
    "skf = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pipeline chaining a standard scaler and a logistic regression model, and evaluate the performance of the model using the function `cross_validate` (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)) which takes as input the cross-validation object created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(\n",
    "    pipe, X, y, scoring='accuracy', cv=skf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that when we pass a pipeline to the `cross_validate` function, both the parameters of the transformer and the model remain fixed when transforming and evaluating the testing data. This prevents leaking data from the training to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the cross-validation procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.03349495, 0.02152586, 0.01249194]),\n",
       " 'score_time': array([0.00093699, 0.00054407, 0.00052118]),\n",
       " 'test_score': array([0.74251497, 0.69461078, 0.60843373])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three values for each entry, which correspond to each iteration of the cross-validation procedure. Generally these test scores are averaged and reported as the final test score (see image above). Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test score: 0.68\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute mean of test scores\n",
    "mean_test_score = cv_results[\"test_score\"].mean()\n",
    "\n",
    "# Print mean test score\n",
    "print(f\"Mean test score: {np.round(mean_test_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also return the training scores and estimators trained in each iteration by setting to `True` the parameters `return_train_score` and `return_estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.02982903, 0.0135119 , 0.01025891]),\n",
       " 'score_time': array([0.00063705, 0.000597  , 0.00062799]),\n",
       " 'estimator': [Pipeline(steps=[('scaler', StandardScaler()), ('clf', LogisticRegression())]),\n",
       "  Pipeline(steps=[('scaler', StandardScaler()), ('clf', LogisticRegression())]),\n",
       "  Pipeline(steps=[('scaler', StandardScaler()), ('clf', LogisticRegression())])],\n",
       " 'test_score': array([0.74251497, 0.69461078, 0.60843373]),\n",
       " 'train_score': array([1., 1., 1.])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross validation\n",
    "cv_results = cross_validate(\n",
    "    pipe, X, y, scoring='accuracy', cv=skf,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cross-validation does not prevent overfitting to the training set, but rather reduces the bias when estimating and reporting the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Can you read the documentation of `RepeatedStratifiedKFold` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold), reflect on how it differs from `StratifiedKFold`, and implement it to train a model on our fake dataset? Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.10710311, 0.08454347, 0.07194901, 0.07881188, 0.06205893,\n",
       "        0.07255793, 0.07950473, 0.07352018, 0.06167197]),\n",
       " 'score_time': array([0.00055075, 0.00034785, 0.0003221 , 0.00036311, 0.0003221 ,\n",
       "        0.00035596, 0.00032115, 0.00031996, 0.000319  ]),\n",
       " 'test_score': array([0.67664671, 0.7005988 , 0.71686747, 0.79640719, 0.68862275,\n",
       "        0.69879518, 0.79640719, 0.70658683, 0.69277108])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Answer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Create cross-validation\n",
    "rsf = RepeatedStratifiedKFold(\n",
    "    n_splits=3, n_repeats=3, random_state=0\n",
    ")\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Cross validate\n",
    "cv_results = cross_validate(\n",
    "    clf, X, y, scoring='accuracy', cv=rsf\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regularization\n",
    "\n",
    "One way of reducing overfitting is to regularize our models. Regularization penalizes to model to avoid it from over-adjusting to the training set. The penalty term is added to the _loss_ function of the model, and the way it is defined determines the type of regularization.\n",
    "\n",
    "The most well-known regularization types are:\n",
    "- __L1 or _Lasso___: Makes the coefficients of the model sparse, meaning some of them are shrinked to 0. This results in the model looking simpler.\n",
    "- __L2 or _Ridge___: Makes the coefficients of the model smaller. This results in the model looking smoother.\n",
    "\n",
    "In this tutorial we won't provide the mathematical formulation of these types of regularization, but you can read them [here](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/05-model-selection.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how _scikit-learn_ regularizes a logistic regression model. Let's create such model and inspect its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2',\n",
       " 'dual': False,\n",
       " 'tol': 0.0001,\n",
       " 'C': 1.0,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'class_weight': None,\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'n_jobs': None,\n",
       " 'l1_ratio': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression()\n",
    "vars(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default scikit-learn penalizes logistic regression models using _Ridge (L2)_ regularization. If you read the documentation of `LogisticRegression` [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) you will also notice that the parameter `C` determines the strenght of the regularization applied. By default this value is set to 1. Lower values will indicate stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know explore how regularizing impacts the values of the coefficients. Using an _L2_ regularization, let's determine how the sum of the absolute value of the coefficients increases with less regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of coefficients with C 0.001: 0.34188132500646984\n",
      "Sum of coefficients with C 0.1: 3.201367995215172\n",
      "Sum of coefficients with C 100: 5.378647816301111\n"
     ]
    }
   ],
   "source": [
    "# List of regularization values to be explored\n",
    "c_values = [0.001, 0.1, 100]\n",
    "\n",
    "# Print sum of absolute values of coefficients\n",
    "for c_val in c_values:\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=c_val).fit(X_train, y_train)\n",
    "    print(f\"Sum of coefficients with C {c_val}: {np.sum(np.abs(clf.coef_))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the values get bigger with less regularization. What happens when we use _L1_ regularization? Think about it before looking at the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of coefficients set to 0 with C 0.001: 20\n",
      "Number of coefficients set to 0 with C 0.1: 18\n",
      "Number of coefficients set to 0 with C 100: 12\n"
     ]
    }
   ],
   "source": [
    "# Print number of coefficients set to 0\n",
    "for c_val in c_values:\n",
    "    clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=c_val).fit(X_train, y_train)\n",
    "    zero_coef = np.sum((clf.coef_)==0)\n",
    "    print(f\"Number of coefficients set to 0 with C {c_val}: {zero_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, increasing the strength of _L1_ regularization increases the number of coefficients that are set to 0 when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Can you explore the [API](https://scikit-learn.org/stable/modules/classes.html) of _scikit-learn_ and determine how a create linear regression model using _L2_ (also called _Ridge_) regularization? Write your answer in the cell below, and press the three dots to reveal the solution.\n",
    "\n",
    "_Hint:_ You will not be able to add regularization to a linear regression model in the same way than the logistic regression case. Check the module [sklearn.linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) for ideas on how to add this regularization instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "__Answer__\n",
    "\n",
    "A linear regression model using _l2_ regularization can be created as follows:\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# _Optional material_: Hyper-parameter tuning\n",
    "\n",
    "The value of regularization that gives the best performance in not known beforehand. Some people search for this value manually, using the whole training and testing dataset multiple times. This could lead to __selection bias__, which in turns results in overestimating the real performance of the model. Instead, good practice would be to tune this parameter the same as the weights of the model. For this we need to understand the concept of __hyper-parameter tuning__.\n",
    "\n",
    "__Hyper-parameters__ are those that define how the model will learn during the training process. The strength of regularization is one example, but also the type of regularization, or any other parameters that get passed to the estimator object before training. This parameters remain fixed when training the coefficients of a model.\n",
    "\n",
    "We can use a special type of cross-validation, called __nested k cross-validation__, where  to \n",
    "\n",
    "\n",
    "This is better illustrated with the following image taken from the excellent blogpost [\"Evaluate a model\"](https://sebastianraschka.com/faq/docs/evaluate-a-model.html) written Sebastian Raschka:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../images/nestedkfold.png\" alt=\"kfoldcv\" width=\"500\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _scikit-learn_ we can implement hyper-parameter tuning with `GridSearchCV` (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_moons(noise=0.352, random_state=1, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=0),\n",
       "             estimator=SVC(degree=5, kernel='poly', random_state=0),\n",
       "             param_grid={'C': [0.001, 0.1, 100]}, scoring='roc_auc')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# List of regularization values to be explored\n",
    "c_values = {\"C\": [0.001, 0.1, 100]}\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# Create cross-validator\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=10, random_state=0\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "search = GridSearchCV(\n",
    "    estimator=svc, param_grid=c_values,\n",
    "    scoring='roc_auc', cv=cv\n",
    ")\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these results to a pandas dataframe for easy exploration of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split93_test_score</th>\n",
       "      <th>split94_test_score</th>\n",
       "      <th>split95_test_score</th>\n",
       "      <th>split96_test_score</th>\n",
       "      <th>split97_test_score</th>\n",
       "      <th>split98_test_score</th>\n",
       "      <th>split99_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 0.001}</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.116808</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.001087      0.000503         0.001303        0.002206   0.001   \n",
       "1       0.000838      0.000188         0.000958        0.000188     0.1   \n",
       "2       0.003048      0.001618         0.000917        0.000136     100   \n",
       "\n",
       "         params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.001}               0.84               0.84               0.68   \n",
       "1    {'C': 0.1}               0.96               0.72               0.68   \n",
       "2    {'C': 100}               0.96               0.72               0.72   \n",
       "\n",
       "   split3_test_score  ...  split93_test_score  split94_test_score  \\\n",
       "0               0.84  ...                0.80                0.72   \n",
       "1               0.88  ...                0.72                0.84   \n",
       "2               0.92  ...                0.68                0.88   \n",
       "\n",
       "   split95_test_score  split96_test_score  split97_test_score  \\\n",
       "0                0.88                0.88                0.80   \n",
       "1                0.92                0.92                0.88   \n",
       "2                0.96                0.96                0.92   \n",
       "\n",
       "   split98_test_score  split99_test_score  mean_test_score  std_test_score  \\\n",
       "0                 1.0                0.72           0.8472        0.116808   \n",
       "1                 1.0                0.76           0.8848        0.096213   \n",
       "2                 1.0                0.56           0.8936        0.110449   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                2  \n",
       "2                1  \n",
       "\n",
       "[3 rows x 109 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(search.cv_results_)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clean up this output in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>0.110449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.096213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>{'C': 0.001}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.116808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               params  rank_test_score  mean_test_score  std_test_score\n",
       "C-value                                                                \n",
       "100        {'C': 100}                1           0.8936        0.110449\n",
       "0.1        {'C': 0.1}                2           0.8848        0.096213\n",
       "0.001    {'C': 0.001}                3           0.8472        0.116808"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort rows by best performance\n",
    "results_df = results_df.sort_values(by=['rank_test_score'])\n",
    "\n",
    "# Create dataframe with results averaged\n",
    "results_df = (\n",
    "    results_df\n",
    "    .set_index(results_df[\"params\"].apply(\n",
    "        lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "    )\n",
    "    .rename_axis(\"C-value\")\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to perform statistical evaluation over these examples [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✏️ Check your knowledge\n",
    "Load the ABIDE 2 dataset and:\n",
    "\n",
    "1. Perform classification analyses using `StratifiedKFold` with 10 folds.\n",
    "    - How variable are the testing scores?\n",
    "    - How different is the accuracy obtained in the training set as compared to the testing set?\n",
    "    - Compute the mean accuracy over folds to obtain a final estimate of the performance of the model.\n",
    "2. If you have read the optional material, use `GridSearchCV` to determine which regularization technique (_L1_ or _L2_) and value of `C` gives the best performance when using `SVC` to perform classification analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources\n",
    "\n",
    "- [Overfitting and underfitting](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/03-overfitting.ipynb), [Validation](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/04-validation.ipynb) and [Model selection](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/05-model-selection.ipynb) by _Tal Yarkoni_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
