{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Preventing overfitting\n",
    "\n",
    "{TO-DO: Overview of notebook}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting/Underfitting and Bias/Variance\n",
    "\n",
    "- !! Explain what is overfitting and underfitting\n",
    "    - Show images\n",
    "- !! Explain Bias and Variance trade-off\n",
    "    - Show images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing data\n",
    "\n",
    "- We need to test the generalizability of our model to real-data generating mechanisms, a process sometimes called __model validation__. For this we need to evaluate the performance of the model when predicting data that has never seen during training (but comes from the same distribution as the training data).\n",
    "- This leads us to the distinction between training and testing data\n",
    "    - We use the training data to train the parameters of the model\n",
    "    - When we test the model using our testing data, we leave these parameters fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can split our data into training and testing set with scikit-learn, and also illustrate the problem of overfitting. \n",
    "\n",
    "First, let's create a fake dataset for classification, and fit and score a support vector classifier that uses a polinomial kernel of degree 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC mean performance: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=100, n_features=20, n_informative=5, n_redundant=15, random_state=1\n",
    ")\n",
    "\n",
    "# Create and fit SVC\n",
    "svc = SVC(kernel='poly', degree=4, random_state=0).fit(X, y)\n",
    "\n",
    "# Score model\n",
    "print(f\"SVC mean performance: {svc.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here and during all the previous examples of this tutorial we have been using the dataset used for training (`X`) to also evaluate the performance of the model. As explained before, this could lead in overestimation of the model perfomance.\n",
    "\n",
    "Let's instead split our dataset into a training and testing set. We can do so using the method `train_test_split` in scikit-learn (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (75, 20)\n",
      "Shape of training labels: (75,)\n",
      "Shape of testing sett: (25, 20)\n",
      "Shape of testing labels: (25,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "print(f\"Shape of training set: {X_train.shape}\")\n",
    "print(f\"Shape of training labels: {y_train.shape}\")\n",
    "print(f\"Shape of testing sett: {X_test.shape}\")\n",
    "print(f\"Shape of testing labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, scikit-learn splits `X` into a training size of 75% and a testing size of 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we fit our estimator to the training set, and evaluate it both using the training and testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC mean accuracy on training data: 0.84\n",
      "SVC mean accuracy on testing data: 0.52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fit model to training set\n",
    "svc = SVC(kernel='poly', degree=5, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model with training and testing data\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(np.arange(len(X_train)), size=25) # subsample training data\n",
    "print(f\"SVC mean accuracy on training data: {svc.score(X_train[idx], y_train[idx])}\")\n",
    "print(f\"SVC mean accuracy on testing data: {svc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performance of the model drops significantly when evaluated on the testing set, indicating that our model may have overfitted to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Suppose we want our testing set size to comprise 20% of the original dataset. Modify `train_test_split` to achieve this aim. Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of testing set: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=0)\n",
    "\n",
    "print(f\"Proportion of testing set: {X_test.shape[0]/len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "Fortunately, there exists various methods that help reducing overfitting during training. In this notebook, we will revise two of them: _cross-validation_ and _regularization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "- One approach for performing model validation and prevent overestimating the performance of the model on real-world data is __cross validation__.\n",
    "- In cross-validation we divide our sample dataset into different subsets. We then perform multiple rounds of training and validating (testing) the model. In each round, we train on some of those subsets, and validate the model on the remaining ones. Importantly, the assignment of the subsets to training and testing sets rotates.\n",
    "- There are many types of cross-validation, but we will use as an example K-Fold cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold cross-validation\n",
    "\n",
    "- One popular type of cross-validation is __K-Fold__ cross validation. In K-Fold cross validation the dataset is split into $k$ equal subsets, also called folds. We then perform `k` rounds where each subset is used to validate the model while the others are used to train the data. Each subset is used for validation only once.\n",
    "- This is better illustrated with the following image:\n",
    "\n",
    "{TO-DO add Image {see for example}}\n",
    "\n",
    "- Another type of cross-validation is __Stratified K-Fold__. It follows the same logic of K-Fold cross-validation, but with this technique we make sure that each fold of the dataset has the same proportion of samples for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement Stratified K-Fold in scikit learn. We will first create some fake data for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=500, n_features=300, n_informative=100, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a stratified cross-validation object using `StratifiedKFold` (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)). We will use 3 folds (defined here as `n_splits`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a logistic regression model and evaluate the model using the cross-validation object defined above and the `cross_validate` method (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    clf, X, y, scoring='accuracy', cv=skf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the cross-validation procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.16224909, 0.08426905, 0.08046103]),\n",
       " 'score_time': array([0.00036287, 0.00050092, 0.00050688]),\n",
       " 'test_score': array([0.79640719, 0.69461078, 0.68072289])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often these test scores are averaged and reported as the final test score. We can do so by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test score: 0.72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_test_score = cv_results[\"test_score\"].mean()\n",
    "print(f\"Mean test score: {np.round(mean_test_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also return the training scores and estimators used in each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    clf, X, y, scoring='accuracy', cv=skf,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.1003089 , 0.08782196, 0.0647471 ]),\n",
       " 'score_time': array([0.00037003, 0.00032902, 0.00031877]),\n",
       " 'estimator': [LogisticRegression(max_iter=1000),\n",
       "  LogisticRegression(max_iter=1000),\n",
       "  LogisticRegression(max_iter=1000)],\n",
       " 'test_score': array([0.79640719, 0.69461078, 0.68072289]),\n",
       " 'train_score': array([1., 1., 1.])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (...) We can see that cross validation does not reduce the overfit, but rather reduces the bias in our reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{TO-DO: show how in the training and testing set we perform the preprocessing steps outside the testing too}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{TO-DO: cross validation and pipeline}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Can you read the documentation of `RepeatedStratifiedKFold` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold), reflect on how it differs from `StratifiedKFold`, and implement it to train a model on our fake dataset? Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00468206, 0.00290632, 0.00316191]),\n",
       " 'score_time': array([0.00042295, 0.00034094, 0.00023603]),\n",
       " 'test_score': array([0.91176471, 0.75757576, 0.6969697 ])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Create cross-validation\n",
    "rsf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Cross validate\n",
    "cv_results = cross_validate(\n",
    "    clf, X, y, scoring='accuracy', cv=skf\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "(...) Add description to this section\n",
    "\n",
    "- !! A way of reducing overfitting is to regularize our models\n",
    "- !! Explain what is regularization\n",
    "    - Regularization penalizes your model during training to avoid overfitting. More specifically, it adds a penalty to the _loss_ function of your model.\n",
    "- There are two main types of regularization:\n",
    "    - L1 or _Lasso_: Makes the coefficients sparse, meaning some of them are shrinked to 0.\n",
    "        - {add formula}\n",
    "        - {the function looks more simple}\n",
    "    - L2 or _Ridge_: Makes the coefficients smaller.\n",
    "        - {add formula}\n",
    "        - {the function looks smoother}\n",
    "- Explain lambda, alpha (hyperparameter)\n",
    "- !! Explain the difference between L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a logistic regression model in scikit-learn and inspect its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2',\n",
       " 'dual': False,\n",
       " 'tol': 0.0001,\n",
       " 'C': 1.0,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'class_weight': None,\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'n_jobs': None,\n",
       " 'l1_ratio': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "vars(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default scikit-learn penalizes logistic regression models using _Ridge (L2)_ regularization. If you read the documentation of `LogisticRegression` [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) you will also notice that the parameter `C` determines the strenght of the regularization applied. By default this value is set to 1. Lower values will indicate stronger regularization.\n",
    "\n",
    "{TO-DO}: explain this in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the sum of the absolute value of the coefficients increases with less regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of coefficients with C 0.001: 0.33325478313438933\n",
      "Sum of coefficients with C 0.1: 3.0971412119716435\n",
      "Sum of coefficients with C 100: 5.122813420602748\n"
     ]
    }
   ],
   "source": [
    "# List of regularization values to be explored\n",
    "c_values = [0.001, 0.1, 100]\n",
    "\n",
    "# Print sum of absolute values of coefficients\n",
    "for c_val in c_values:\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=c_val).fit(X_train, y_train)\n",
    "    print(f\"Sum of coefficients with C {c_val}: {np.sum(np.abs(clf.coef_))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...) What happens when we use _l1_ regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of coefficients set to 0 with C 0.001: 20\n",
      "Number of coefficients set to 0 with C 0.1: 18\n",
      "Number of coefficients set to 0 with C 100: 11\n"
     ]
    }
   ],
   "source": [
    "# Print number of coefficients set to 0\n",
    "for c_val in c_values:\n",
    "    clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=c_val).fit(X_train, y_train)\n",
    "    zero_coef = np.sum((clf.coef_)==0)\n",
    "    print(f\"Number of coefficients set to 0 with C {c_val}: {zero_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explore the [API](https://scikit-learn.org/stable/modules/classes.html) of scikit-learn and determine how a create linear regression model using _l2_ (also called _Ridge_) regularization? Write your answer in the cell below, and press the three dots to reveal the solution.\n",
    "\n",
    "(_Hint:_ It will not be able to add regularization in the same way as with a logistic regression model. Check the module [sklearn.linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "__Answer__\n",
    "\n",
    "A linear regression model using _l2_ regularization can be created as follows:\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyper-parameter tuning\n",
    "\n",
    "The value of regularization that gives the best performance in not known beforehand. Some people search the best performance value manually using the whole training and testing dataset. This could lead to overestimating the real performance of the model. Instead, good practice would be to tune this parameter the same as the weights of the model. This leads us to understanding the concept of __hyper-parameter tuning__.\n",
    "\n",
    "(...) add description to this section\n",
    "- What are hyperparameters\n",
    "    - don't change during training\n",
    "- Why we should tune our hyperparametes using cross validation and not search them manually.\n",
    "    - concept of __selection bias__\n",
    "- Concept of Nested Cross Validation\n",
    "    - Image {see https://sebastianraschka.com/faq/docs/evaluate-a-model.html}\n",
    "- What is, and how to perform, `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(noise=0.352, random_state=1, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': ['linear']},\n",
    "    {'kernel': ['poly'], 'degree': [2, 3]},\n",
    "    {'kernel': ['rbf']}\n",
    "]\n",
    "\n",
    "svc = SVC(random_state=0)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=10, random_state=0\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=svc, param_grid=param_grid,\n",
    "    scoring='roc_auc', cv=cv\n",
    ")\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(search.cv_results_)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.sort_values(by=['rank_test_score'])\n",
    "results_df = (\n",
    "    results_df\n",
    "    .set_index(results_df[\"params\"].apply(\n",
    "        lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "    )\n",
    "    .rename_axis('kernel')\n",
    ")\n",
    "results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to perform statistical evaluation over these examples [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Can you use `GridSearchCV` to search for the best value of `C` for a logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your knowledge\n",
    "Load the ABIDE 2 dataset and:\n",
    "\n",
    "1. Perform classification analyses using `StratifiedKFold` with 10 folds.\n",
    "    - How variable are the testing scores?\n",
    "    - How different is the accuracy obtained in the training set as compared to the testing set?\n",
    "    - Compute the mean accuracy over folds to obtain a final estimate of the performance of the model.\n",
    "2. Use GridSearchCV to determine which regularization technique (l1 or l2) and value of `C` gives the best performance when using `SVC` to perform classification analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
