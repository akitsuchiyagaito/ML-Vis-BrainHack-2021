{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Disclaimer: This notebook is based and reproduces information from chapter 10 of the book [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. All rights of the material belong to the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PRINCIPAL COMPONENT ANALYSIS (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PCA is an **unsupervised approach**:\n",
    "    \n",
    "    - We have only a set of features measured on n observations, but no response Y\n",
    "    \n",
    "    - This means we cannot easily assess the accuracy of the results obtained \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PCA finds a **low-dimensional representation** of the data that captures as much of the information as possible\n",
    "\n",
    "    - Reduces an n x p data matrix by reducing the size of p (variables, also called features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Principal component analysis (PCA) refers to the process by which **principal components** are computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a principal component?\n",
    "\n",
    "- The first principal component direction of the data is that along which the observations vary the most.\n",
    "\n",
    "\n",
    "- Each of the component found by PCA is a **linear combination of the p features**:\n",
    "\n",
    "    ![image_linear_combination.png](./images/linear_combination_pca.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loading vectors\n",
    "\n",
    "- The elements φ11, . . . , φp1 are the loadings of the first principal component\n",
    "\n",
    "- Together, the loadings make up the principal component loading vector:\n",
    "\n",
    "    ![image_loading_vectors.png](./images/loading_vectors.png)\n",
    "\n",
    "\n",
    "\n",
    "- The loading vectors are normalized -> their sum of squares is equal to 1\n",
    "\n",
    "    ![image_normalized_loading_vectors.png](./images/normalized_loading_vectors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Principal component scores\n",
    "- The values of z11, . . . , zn1 are the principal component scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Geometric interpretation\n",
    "- Principal component loading vectors -> directions in feature space along which the data vary the most\n",
    "\n",
    "\n",
    "- Principal component scores -> projections along these direction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative interpretation of the principal components \n",
    "- The first principal component vector defines the line that is as close as possible to the original observations \n",
    "    \n",
    "    - It minimizes the sum of squared distances from each point to the plane\n",
    "\n",
    "        ![image_pca_closeness.png](./images/pca_closeness.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we compute the first principal component?\n",
    "\n",
    "1. Since we are only interested in variance, we assume that each of the variables in X has been centered to have mean zero \n",
    "\n",
    "\n",
    "2. We then look for the linear combination of the sample feature values that has the largest variance, which solves the optimization problem: \n",
    "\n",
    "    ![image_optimization_pca.png](./images/optimization_pca.png)\n",
    "\n",
    "\n",
    "3. We then look for the second principal component, which is the linear combination of X1, . . . , Xp that has maximal variance out of all linear combinations that are uncorrelated/orthogonal/perpendicular with Z1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use the dataset USA-ARRESTS to put into practice these concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's import and print the USA arrest data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load and print USA arrest data\n",
    "usarrest_df = pd.read_csv('../data/usa_arrest.csv')\n",
    "print(usarrest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each of the 50 states in the United States, the data set contains the number of arrests per 100, 000 residents for each of three crimes: Assault, Murder, and Rape. It also contains the percent of the population in each state living in urban areas.\n",
    "\n",
    "Notice that the variables are measured in different units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore the mean, variance and standard deviation of each variable (feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print mean value of each feature\n",
    "usarrest_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print variance value of each feature\n",
    "usarrest_df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print std of each feature\n",
    "usarrest_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can observe that features differ greatly in their mean, variance and standard deviation. \n",
    "\n",
    "This is information to keep in mind when performing principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, let's put the data into the right formats for performing PCA. We will begin by creating separate arrays containing the values of the observations and the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create array of states\n",
    "states = usarrest_df.index.get_values()\n",
    "\n",
    "# Create arrays of features\n",
    "features = usarrest_df.columns.get_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will also convert the whole dataset to numpy array to give it as input to scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to numpy array\n",
    "usarrest = usarrest_df.loc[:, features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The results obtained when we perform PCA will depend on whether the variables have been individually scaled (each multiplied by a different constant)\n",
    "\n",
    "\n",
    "- In our case, if we do not scale the data, the principal component loading vector will have a very large loading for Assault, since that variable has by far the highest variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's convert the data to have mean 0 and std 1, since feature's values are in different scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Standarize dataset\n",
    "scaler = StandardScaler()\n",
    "usarrest_scaled = scaler.fit_transform(usarrest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perform PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, let's perform PCA over the scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca_score_vectors = pca.fit_transform(usarrest_scaled)   # store score vectors in new variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine each PCA loading vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print PCA loadings\n",
    "columns_pca = ['PCA_' + str(pca +1) for pca in range(0, np.shape(pca_score_vectors)[1])]\n",
    "pca_loadings = (pca.components_).T\n",
    "pca_loadings_df = pd.DataFrame(pca_loadings,\n",
    "                               index=features, \n",
    "                               columns=columns_pca)\n",
    "print(pca_loadings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine each PCA score vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print principal component score vectors\n",
    "pca_score_vectors_df = pd.DataFrame(pca_score_vectors, \n",
    "                                    index=states, \n",
    "                                    columns=columns_pca)\n",
    "print(pca_score_vectors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a sanity check, let's make sure that the product of our scaled dataset and the loadings of the first component returns the score vectors that we obtained for the first component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute dot product between X and loadings of the first component\n",
    "pca1_score_vectors = np.dot(usarrest_scaled, pca_loadings[:,0])\n",
    "print(pca1_score_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We observe that our vector is the same as that returned by scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plot PCA results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A biplot is a useful way of visualizing the results of PCA\n",
    "\n",
    "\n",
    "- It displays both the principal component scores and the principal component loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's make a biplot of the first two components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Biplot of the first two components\n",
    "fig, ax1 = plt.subplots(figsize=(7, 7));\n",
    "\n",
    "rect = 0, 0, 1, 1  \n",
    "\n",
    "# plot score points\n",
    "ax1 = fig.add_axes(rect)\n",
    "\n",
    "ax1.scatter(pca_score_vectors[:,0], pca_score_vectors[:,1],\n",
    "           alpha=0.3)\n",
    "for i in range(pca_score_vectors.shape[0]):\n",
    "    ax1.text(pca_score_vectors[i,0], pca_score_vectors[i,1], states[i],\n",
    "             alpha=0.8)\n",
    "\n",
    "min_sv = np.min(pca_score_vectors[:,0:2])\n",
    "max_sv = np.max(pca_score_vectors[:,0:2])\n",
    "ax1.set_xlim(min_sv - 0.5, max_sv + 0.5)\n",
    "ax1.set_ylim(min_sv - 0.5, max_sv + 0.5)\n",
    "\n",
    "ax1.set_xlabel('PCA_1')\n",
    "ax1.set_ylabel('PCA_2')\n",
    "\n",
    "# plot loading vectors\n",
    "ax2 = fig.add_axes(rect,  frameon=False)\n",
    "\n",
    "for i in range(pca_loadings.shape[0]):\n",
    "    ax2.arrow(0, 0, pca_loadings[i,0], pca_loadings[i,1], \n",
    "              color = 'r', alpha = 1,\n",
    "              head_width=0.02, head_length=0.02)\n",
    "    ax2.text(pca_loadings[i,0]* 1.2,  pca_loadings[i,1] * 1.1, features[i], \n",
    "             color = 'r', ha = 'center', va = 'center')\n",
    "    \n",
    "ax2.set_xlim(-1,1)\n",
    "ax2.set_ylim(-1,1)\n",
    "\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_ticks([-0.5, 0, 0.5])\n",
    "ax2.yaxis.set_label_position('right')\n",
    "ax2.xaxis.tick_top()\n",
    "ax2.xaxis.set_ticks([-0.5, 0, 0.5])\n",
    "ax2.xaxis.set_label_position('top')\n",
    "\n",
    "ax2.grid()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In chapter 10, the authors intepret from the plot the following: \n",
    "\n",
    "- The first loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight on UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. \n",
    "\n",
    "- The second loading vector places most of its weight on UrbanPop and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanization of the state.\n",
    "\n",
    "- Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.\n",
    "\n",
    "\n",
    "- States with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. \n",
    "\n",
    "- California also has a high score on the second component, indicating a high level of urbanization, while the opposite is true for states like Mississippi. \n",
    "\n",
    "- States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compute the proportion of variance explained by each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Important to know how much of the information in a given data set is lost by projecting the observations onto the first few principal components\n",
    "\n",
    "\n",
    "- This can be answered by knowing the proportion of variance explained (PVE) by each component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The proportion of the variance explained by the mth component is the variance explained by the mth component divided by the total variance of the dataset \n",
    "\n",
    "- The variance explained by the _m_th component is defined as:\n",
    "\n",
    "    ![image_variance_component.png](../images/variance_component.png)\n",
    "    \n",
    "- The total variance of a dataset is defined as:\n",
    "\n",
    "    ![image_total_variance.png](../images/total_variance.png)\n",
    "\n",
    "- Therefore, the PVE is given by: \n",
    "\n",
    "    ![image_PVE.png](../images/PVE.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compute the variance explained, and the proportion of variance explained, of each component with sci-kit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print explained variance by each component\n",
    "variance = pca.explained_variance_\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "variance_df = pd.DataFrame([variance, variance_ratio], \n",
    "                           index = ['Variance', 'Variance_ratio'], \n",
    "                           columns=columns_pca)\n",
    "print(variance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot PVE's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We typically decide on the number of principal components required to visualize the data by examining a scree plot. \n",
    "\n",
    "\n",
    "- This is done by eyeballing the scree plot, and looking for a point at which the proportion of variance explained by each subsequent principal component drops off (often refered as the elbow in the scree plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the PVEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot variance explained by each component \n",
    "plt.plot(variance_df.loc['Variance_ratio'])\n",
    "plt.title('Proportion of variance explained by each component')\n",
    "plt.ylabel('Variance Ratio');\n",
    "plt.xlabel('Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cumulative sum of PVE's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also decide to examine the cumulative sum of PVE's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot cumulative sum of variance explained by each component\n",
    "cumulative_values = np.cumsum(variance_ratio)\n",
    "\n",
    "plt.plot(cumulative_values)\n",
    "plt.title('Cumulative proportion of variance explained')\n",
    "plt.ylabel('Variance Ratio');\n",
    "plt.xlabel('Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other uses of PCA\n",
    "\n",
    "- Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization\n",
    "\n",
    "- We can also use the principal component score vectors as features for other analyses, such as regression, classification, and clustering (using as input the n × M matrix) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
